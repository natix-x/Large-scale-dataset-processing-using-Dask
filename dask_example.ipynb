{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dask client initialization",
   "id": "8f0ea5357e2a2829"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T19:19:28.074641Z",
     "start_time": "2025-06-04T19:19:28.070899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=6, threads_per_worker=2, memory_limit='2GB')"
   ],
   "id": "c9ebbf5d29371d2c",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Create a cluster on AWS\n",
    "# from dask_cloudprovider.aws import EC2Cluster\n",
    "#\n",
    "# aws_key_pair_name = \"YOUR_AWS_KEY_PAIR_NAME\"\n",
    "# cluster = EC2Cluster(instance_type=\"t3.micro\", n_workers=1, key_name=aws_key_pair_name, region=\"us-east-1\")\n",
    "# client = Client(cluster)"
   ],
   "id": "94a6b25d3e4c0684",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T18:59:57.473360Z",
     "start_time": "2025-06-04T18:59:57.468834Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Dashboard:\", client.dashboard_link)",
   "id": "691fdb5391db7dd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load csv data from s3",
   "id": "58bf4279329c141a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import dask.dataframe as dd\n",
    "import s3fs\n",
    "\n",
    "base_s3_path_dir = \"synthea-open-data/coherent/unzipped/csv/\"\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "all_s3_files = fs.ls(base_s3_path_dir)\n",
    "csv_files =[]\n",
    "for file in all_s3_files:\n",
    "    if file.endswith('.csv'):\n",
    "        csv_files.append(file)\n",
    "\n",
    "no_of_files = len(csv_files)\n",
    "\n",
    "# in pandas dtype overrides were not needed\n",
    "DTYPE_OVERRIDES = {\n",
    "    'allergies.csv': {'STOP': 'object'},\n",
    "    'patients.csv': {'SUFFIX': 'object'},\n",
    "    'observations.csv': {'VALUE': 'object'},\n",
    "    'organizations.csv': {'PHONE': 'object', 'ZIP': 'object'},\n",
    "    'procedures.csv': {'REASONDESCRIPTION': 'object'},\n",
    "    'providers.csv': {'PHONE': 'object', 'ZIP': 'object'}}\n",
    "\n",
    "for filepath in csv_files:\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    print(f\"\\nFile: {filename} - {csv_files.index(filepath)+1} out of {no_of_files}\")\n",
    "\n",
    "    file_dtypes = DTYPE_OVERRIDES.get(filename, {})\n",
    "    ddf = dd.read_csv(f\"s3://{filepath}\", storage_options={'anon': True}, dtype=file_dtypes)\n",
    "\n",
    "    output_file = \"parquet_files/\" + filename.replace('.csv', '.parquet')\n",
    "    ddf.to_parquet(output_file, write_index=False, overwrite=True)"
   ],
   "id": "95149176fe6be9f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T21:44:36.201845Z",
     "start_time": "2025-06-04T21:44:33.771827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def process_data(library):\n",
    "    start = time.time()\n",
    "\n",
    "    if library == \"dask\":\n",
    "        df_obs = dd.read_parquet(\"parquet_files/obs.parquet\")\n",
    "        df_pat = dd.read_parquet(\"parquet_files/patients.parquet\")\n",
    "    else:\n",
    "        df_obs = pd.read_parquet(\"parquet_files/obs.parquet\")\n",
    "        df_pat = pd.read_parquet(\"parquet_files/patients.parquet\")\n",
    "\n",
    "    wanted_codes = [\"8302-2\", \"29463-7\", \"8462-4\", \"8480-6\"]\n",
    "    df_obs = df_obs[df_obs[\"CODE\"].isin(wanted_codes)]\n",
    "\n",
    "    if library == \"dask\":\n",
    "        df_obs[\"VALUE\"] = dd.to_numeric(df_obs[\"VALUE\"], errors=\"coerce\")\n",
    "        df_pat[\"BIRTHDATE\"] = dd.to_datetime(df_pat[\"BIRTHDATE\"], errors=\"coerce\")\n",
    "\n",
    "    else:\n",
    "        df_obs[\"VALUE\"] = pd.to_numeric(df_obs[\"VALUE\"], errors=\"coerce\")\n",
    "        df_pat[\"BIRTHDATE\"] = pd.to_datetime(df_pat[\"BIRTHDATE\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "    df_pat[\"AGE\"] = datetime.now().year - df_pat[\"BIRTHDATE\"].dt.year\n",
    "    df_pat = df_pat[(df_pat[\"GENDER\"] == \"F\") & (df_pat[\"AGE\"] > 40)][[\"Id\", \"AGE\"]]\n",
    "\n",
    "    df_merged = df_obs.merge(df_pat, left_on=\"PATIENT\", right_on=\"Id\", how=\"inner\")\n",
    "\n",
    "    result = df_merged.groupby([\"PATIENT\", \"CODE\"])[\"VALUE\"].mean()\n",
    "\n",
    "    if library == \"dask\":\n",
    "        result = result.compute()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    return result, round(end - start, 2)\n",
    "\n",
    "pandas_result, pandas_time = process_data(\"pandas\")\n",
    "dask_result, dask_time = process_data(\"dask\")\n",
    "print(f\"Pandas processing time: {pandas_time} seconds\")\n",
    "print(f\"Dask processing time: {dask_time} seconds\")"
   ],
   "id": "325a6a0a512e6d11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas processing time: 1.92 seconds\n",
      "Dask processing time: 0.45 seconds\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T21:16:39.412330Z",
     "start_time": "2025-06-04T21:16:35.366333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def count_total_rows(library):\n",
    "    directory = \"parquet_files\"\n",
    "    all_files = []\n",
    "    for f in os.listdir(directory):\n",
    "        all_files.append(os.path.join(directory, f))\n",
    "\n",
    "    start = time.time()\n",
    "    total_rows = 0\n",
    "    for file in all_files:\n",
    "        if library == 'dask':\n",
    "            ddf = dd.read_parquet(file)\n",
    "            row_count = len(ddf)\n",
    "        else:\n",
    "            df = pd.read_parquet(file)\n",
    "            row_count = len(df)\n",
    "        total_rows += row_count\n",
    "    end = time.time()\n",
    "    return total_rows, round(end - start, 2)\n",
    "\n",
    "pandas_total, pandas_time = count_total_rows(\"pandas\")\n",
    "dask_total, dask_time = count_total_rows(\"dask\")\n",
    "print(f\"Pandas processing time: {pandas_time} seconds\")\n",
    "print(f\"Dask processing time: {dask_time} seconds\")"
   ],
   "id": "823f33ad2d24e219",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas processing time: 3.75 seconds\n",
      "Dask processing time: 0.28 seconds\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T21:39:58.146162Z",
     "start_time": "2025-06-04T21:39:58.142279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.close()\n",
    "# cluster.close()"
   ],
   "id": "9fc327f39ec848db",
   "outputs": [],
   "execution_count": 119
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
